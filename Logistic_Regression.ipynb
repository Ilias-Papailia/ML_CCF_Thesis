{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report,f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "import pickle\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.dummy import DummyClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')  \n",
    "scaler = StandardScaler()\n",
    "\n",
    "df['Amount'] = scaler.fit_transform(df[['Amount']])\n",
    "df['Time'] = scaler.fit_transform(df[['Time']])\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN SPLIT 80 % 20 % , HYPERPARAMETER TUNING FOR LOGISTIC REGRESSION, STRATIFIED FOLD FOR VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "param_grid_cost = {\n",
    "    'C': [0.1,1,10], \n",
    "    'penalty': ['l1','l2'],  \n",
    "    'solver': ['liblinear','saga'], \n",
    "     'class_weight': [\"balanced\",{0: 1, 1: 1.5},{0: 1, 1: 1}, {0:1,1:2} , {0: 1, 1: 10},{0: 1, 1: 3},{0: 1, 1: 4},{0: 1, 1: 5},{0: 1, 1: 15},{0: 1, 1: 20}]\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1,1,10], \n",
    "    'penalty': ['l1','l2'],  \n",
    "    'solver': ['liblinear','saga'], \n",
    "}\n",
    "\n",
    "param_grid_empty ={}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION TO EVALUATE PERFORMANCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(logistic, X_test, y_test):\n",
    "    y_pred = logistic.predict(X_test)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "    gmean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"f1-score: {f1}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Classification Report:\\n{classification_rep}\")\n",
    "    print(f\"Geometric Mean: {gmean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION FOR P R CURVE :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRCurve(model,X_test,y_test,title):\n",
    "    y_scores = model.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "    auprc = auc(recall, precision)\n",
    "    print(f\"Area Under the Precision-Recall Curve (AUPRC): {auprc}\")\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label='Precision-Recall curve',color='red')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DUMMY CLASSIFIER BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"uniform\",random_state =42)\n",
    "dummy_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(dummy_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCurve(dummy_clf,X_test,y_test,\"PR Curve DummyClassifier Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(log, param_grid, cv=cv, scoring='f1',n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_baseline = grid_search.best_estimator_\n",
    "\n",
    "dump(logistic_baseline, 'Models/lr_Baseline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_baseline = load('Models/lr_Baseline.joblib')\n",
    "evaluate_model_performance(logistic_baseline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCurve(logistic_baseline,X_test,y_test,\"PR Curve Hyperparameter Tuned Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UNDERSAMPLING TECHNIQUES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomek Links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Before Tomek Links undersampling: {X_train.shape[0]} samples')\n",
    "tl = TomekLinks(n_jobs=-1) \n",
    "X_res, y_res = tl.fit_resample(X_train, y_train)\n",
    "print(f'After Tomek Links undersampling: {X_res.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(random_state=42)\n",
    "grid_search_tl = GridSearchCV(log, param_grid, cv=cv, scoring='f1', verbose=1,n_jobs=-1)\n",
    "grid_search_tl.fit(X_res, y_res)\n",
    "grid_search_tl.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resampled_Tomek.pkl', 'wb') as file:\n",
    "    pickle.dump((X_res, y_res), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic_TomekLinks = grid_search_tl.best_estimator_\n",
    "dump(Logistic_TomekLinks, 'Models/lr_TomekLink.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_Tomek = load('Models/lr_TomekLink.joblib')\n",
    "evaluate_model_performance(logistic_Tomek, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCurve(logistic_Tomek,X_test,y_test,\"PR Curve Logistic Regression Tomek LinK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Near Miss 3 Implementations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearMiss1 = NearMiss(version=1,n_jobs=-1,n_neighbors=5)\n",
    "\n",
    "X_resampled_nm1, Y_resampled_nm1 = nearMiss1.fit_resample(X_train, y_train)\n",
    "\n",
    "grid_search_nm1 = GridSearchCV(log, param_grid, cv=cv, scoring='f1', verbose=1,n_jobs=-1)\n",
    "grid_search_nm1.fit(X_resampled_nm1,Y_resampled_nm1)\n",
    "\n",
    "print(f\"Before resampling: {Counter(y_train)}\")\n",
    "print(f\"After resampling with NearMiss-1: {Counter(Y_resampled_nm1)}\")\n",
    "\n",
    "grid_search_nm1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_NearMiss1 = grid_search_nm1.best_estimator_\n",
    "dump(logistic_NearMiss1, 'Models/lr_NearMiss1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_NearMiss1 = load('Models/lr_NearMiss1.joblib')\n",
    "evaluate_model_performance(logistic_NearMiss1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCurve(logistic_NearMiss1,X_test,y_test,\"PR Curve Logistic Regression NearMiss(1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Near Miss 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearMiss2 = NearMiss(version=2,n_jobs=-1,n_neighbors=5)\n",
    "log = LogisticRegression(random_state=42)\n",
    "\n",
    "X_resampled_nm2, Y_resampled_nm2 = nearMiss2.fit_resample(X_train, y_train)\n",
    "\n",
    "grid_search_nm2 = GridSearchCV(log, param_grid, cv=cv, scoring='f1', verbose=1,n_jobs=-1)\n",
    "grid_search_nm2.fit(X_resampled_nm2,Y_resampled_nm2)\n",
    "\n",
    "grid_search_nm2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_NearMiss2 = grid_search_nm2.best_estimator_\n",
    "dump(logistic_NearMiss2, 'Models/lr_NearMiss2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_NearMiss2 = load('Models/lr_NearMiss2.joblib')\n",
    "evaluate_model_performance(logistic_NearMiss2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCurve(logistic_NearMiss2,X_test,y_test,\"PR Curve Logistic Regression NearMiss(2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Near Miss 3 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearMiss3 = NearMiss(version=3,n_jobs=-1,n_neighbors=5)\n",
    "log = LogisticRegression(random_state=42)\n",
    "\n",
    "X_resampled_nm3, Y_resampled_nm3 = nearMiss3.fit_resample(X_train, y_train)\n",
    "\n",
    "grid_search_nm3 = GridSearchCV(log, param_grid, cv=cv, scoring='f1', verbose=1,n_jobs=-1)\n",
    "grid_search_nm3.fit(X_resampled_nm3,Y_resampled_nm3)\n",
    "grid_search_nm3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_NearMiss3 = grid_search_nm3.best_estimator_\n",
    "dump(logistic_NearMiss3, 'Models/lr_NearMiss3.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_NearMiss3 = load('Models/lr_NearMiss3.joblib')\n",
    "evaluate_model_performance(logistic_NearMiss3, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCurve(logistic_NearMiss3,X_test,y_test,\"PR Curve Logistic Regression NearMiss(3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERSAMPLING TECHNIQUES:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Features shape: {X_train.shape}\")\n",
    "smote = SMOTE(random_state=42, n_jobs=-1)\n",
    "\n",
    "log = LogisticRegression(random_state=42)\n",
    "\n",
    "X_resampled_smote, Y_resampled_smote = smote.fit_resample(X_train, y_train)\n",
    "print(f\"Features shape: {X_resampled_smote.shape}\")\n",
    "\n",
    "grid_search_smote = GridSearchCV(log, param_grid, cv=cv, scoring='f1', verbose=1,n_jobs=-1)\n",
    "grid_search_smote.fit(X_resampled_smote,Y_resampled_smote)\n",
    "grid_search_smote.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_smote = grid_search_smote.best_estimator_\n",
    "dump(logistic_smote, 'Models/lr_Smote.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Features shape: {X_train.shape}\")\n",
    "print(f\"Target shape: {y_train.shape}\")\n",
    "print(\"\\nAfter SMOTE:\")\n",
    "\n",
    "print(f\"Features shape: {X_resampled_smote.shape}\")\n",
    "print(f\"Target shape: {Y_resampled_smote.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_SMOTE = load('Models/lr_Smote.joblib')\n",
    "evaluate_model_performance(logistic_SMOTE, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCurve(logistic_SMOTE,X_test,y_test,\"PR Curve Logistic Regression SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADASYN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Features shape: {X_train.shape}\")\n",
    "adasyn = ADASYN(random_state=42, n_jobs=-1)\n",
    "log = LogisticRegression(random_state=42)\n",
    "\n",
    "X_resampled_ad, Y_resampled_ad = adasyn.fit_resample(X_train, y_train)\n",
    "print(f\"Features shape: {X_resampled_ad.shape}\")\n",
    "\n",
    "grid_search_ad = GridSearchCV(log, param_grid, cv=cv, scoring='f1', verbose=1,n_jobs=-1)\n",
    "grid_search_ad.fit(X_resampled_ad,Y_resampled_ad)\n",
    "grid_search_ad.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(grid_search_ad.best_estimator_, 'Models/lr_adasyn.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_adasyn = load('Models/lr_adasyn.joblib')\n",
    "evaluate_model_performance(logistic_adasyn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCurve(logistic_adasyn,X_test,y_test,\"PR Curve Logistic Regression ADASYN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS AN EXAMPLE OF HOW YOU CAN USE THE PR CRUVE TO MAKE AN ANALYSIS ON YOUR MODEL. For this example we can see that if we find the best precision Recall Curve we can find a really balanced model. But in this case we have to take into account that Recall is much more important since FN are much more punishing. I will discuss this in my thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = logistic_adasyn.predict_proba(X_test)[:, 1] \n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "f1_scores = [f1_score(y_test, y_scores > t) for t in thresholds]\n",
    "\n",
    "best_index = np.argmax(f1_scores)\n",
    "\n",
    "best_threshold = thresholds[best_index]\n",
    "\n",
    "predictions = y_scores > best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, predictions, target_names=['Negative', 'Positive'])\n",
    "\n",
    "print('Best Threshold:', best_threshold)\n",
    "print(report)\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(conf_matrix)\n",
    "mcc = matthews_corrcoef(y_test, predictions)\n",
    "print(mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BORDERLINE SMOTE 2 APPROACHES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BORDERLINE SMOTE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Features shape: {X_train.shape}\")\n",
    "\n",
    "borderline_smote = BorderlineSMOTE(random_state=42, n_jobs=-1)\n",
    "\n",
    "log = LogisticRegression(random_state=42)\n",
    "X_resampled_bs, Y_resampled_bs = borderline_smote.fit_resample(X_train, y_train)\n",
    "print(f\"Features shape: {X_resampled_bs.shape}\")\n",
    "\n",
    "grid_search_bs = GridSearchCV(log, param_grid, cv=cv, scoring='f1', verbose=1, n_jobs=-1)\n",
    "grid_search_bs.fit(X_resampled_bs, Y_resampled_bs)\n",
    "grid_search_bs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(grid_search_bs.best_estimator_, 'Models/lr_borderlinesmote1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_Bsmote = load('Models/lr_borderlinesmote1.joblib')\n",
    "evaluate_model_performance(logistic_Bsmote, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCurve(logistic_Bsmote,X_test,y_test,\"PR Curve Logistic Regression BorderlineSMOTE(1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BORDERLINE SMOTE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Features shape: {X_train.shape}\")\n",
    "\n",
    "borderline_smote2 = BorderlineSMOTE(random_state=42, n_jobs=-1,kind='borderline-2')\n",
    "\n",
    "log = LogisticRegression(random_state=42)\n",
    "\n",
    "X_resampled_bs2, Y_resampled_bs2 = borderline_smote2.fit_resample(X_train, y_train)\n",
    "print(f\"Features shape: {X_resampled_bs2.shape}\")\n",
    "\n",
    "grid_search_bs2 = GridSearchCV(log, param_grid, cv=cv, scoring='f1', verbose=1, n_jobs=-1)\n",
    "grid_search_bs2.fit(X_resampled_bs2, Y_resampled_bs2)\n",
    "grid_search_bs2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(grid_search_bs2.best_estimator_, 'Models/lr_borderlinesmote2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_Bsmote2 = load('Models/lr_borderlinesmote2.joblib')\n",
    "evaluate_model_performance(logistic_Bsmote2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCurve(logistic_Bsmote2,X_test,y_test,\"PR Curve Logistic Regression BorderlineSMOTE(2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HYBRID APPROACHES :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Features shape: {X_train.shape}\")\n",
    "\n",
    "smote_tomek = SMOTETomek(random_state=42, n_jobs=-1)\n",
    "log = LogisticRegression(random_state=42)\n",
    "\n",
    "X_resampled_st, Y_resampled_st = smote_tomek.fit_resample(X_train, y_train)\n",
    "print(f\"Features shape: {X_resampled_st.shape}\")\n",
    "\n",
    "grid_search_st = GridSearchCV(log, param_grid, cv=cv, scoring='f1', verbose=1, n_jobs=-1)\n",
    "grid_search_st.fit(X_resampled_st, Y_resampled_st)\n",
    "grid_search_st.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the Resampled parts since they take a long time to compute (took 1 hour..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resampled_data.pkl', 'wb') as file:\n",
    "    pickle.dump((X_resampled_st, Y_resampled_st), file)\n",
    "\n",
    "# if you want to read\n",
    "#with open('resampled_data.pkl', 'rb') as file:\n",
    "#    X_loaded, Y_loaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(grid_search_st.best_estimator_, 'Models/lr_smotetomek.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_smoketomek = load('Models/lr_smotetomek.joblib')\n",
    "evaluate_model_performance(logistic_smoketomek,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCurve(logistic_smoketomek,X_test,y_test,\"PR Curve Logistic Regression SMOTETomek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resampled_data.pkl', 'rb') as file:\n",
    "    X_loaded, Y_loaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTEENN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Features shape: {X_train.shape}\")\n",
    "\n",
    "smote_enn = SMOTEENN(random_state=42, n_jobs=-1)\n",
    "log = LogisticRegression(random_state=42)\n",
    "\n",
    "X_resampled_se, Y_resampled_se = smote_enn.fit_resample(X_train, y_train)\n",
    "print(f\"Features shape after SMOTEENN: {X_resampled_se.shape}\")\n",
    "\n",
    "grid_search_se = GridSearchCV(log, param_grid, cv=cv, scoring='f1', verbose=1, n_jobs=-1)\n",
    "grid_search_se.fit(X_resampled_se, Y_resampled_se)\n",
    "grid_search_se.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resampled_dataSmoteE.pkl', 'wb') as file:\n",
    "    pickle.dump((X_resampled_se, Y_resampled_se), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(grid_search_se.best_estimator_, 'Models/lr_smoteE.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_smotee = load( 'Models/lr_smoteE.joblib')\n",
    "evaluate_model_performance(linear_smotee,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCurve(linear_smotee,X_test,y_test,\"PR Curve Logistic Regression SMOTEENN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COST SENSITIVE LEARNING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log = LogisticRegression(random_state=42)\n",
    "\n",
    "grid_search_cost = GridSearchCV(log, param_grid_cost, cv=cv, scoring='f1',n_jobs=-1)\n",
    "\n",
    "grid_search_cost.fit(X_train, y_train)\n",
    "grid_search_cost.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(grid_search_cost.best_estimator_, 'Models/lr_costsensitive.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_costSensitive = load( 'Models/lr_costsensitive.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(linear_costSensitive,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCurve(linear_costSensitive,X_test,y_test,\"PR Curve Logistic Regression Cost Sensitive Learning \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS TO TEST SMOTE TOMEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 2: Apply Tomek Links\n",
    "tl = TomekLinks()\n",
    "X_tl, y_tl = tl.fit_resample(X_smote, y_smote)\n",
    "\n",
    "print(f\"Original dataset shape: {Counter(y)}\")\n",
    "print(f\"After SMOTE: {Counter(y_smote)}\")\n",
    "print(f\"After Tomek Links: {Counter(y_tl)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
